{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM - Kernel\n",
    "\n",
    "* Potentielle Kernel: RBF, Polynomial, Spectral Mixture Kernel, Sigmoid Kernel\n",
    "* Calculate SVM using dual decision functions\n",
    "* Thus: Model theta has as many parameters as examples.\n",
    "* Kernel function: Measure of similarity between instances.\n",
    "* Hence: How similar is instance x to each other trainings instance?\n",
    "* Derivation from the primal into the dual form is necessary (lecture).\n",
    "\n",
    "Optimization criterion of the dual SVM:\n",
    "\n",
    "$$\n",
    "max_{\\beta} \\sum_{i=1}^n \\beta_i - \\frac{1}{2} \\sum_{i,j=1}^n \\beta_i \\beta_j y_i y_j k(x_i,x_j) \\text{, such that } 0 \\leq \\beta_i \\leq \\lambda\n",
    "$$\n",
    "\n",
    "* Optimization over parameters beta\n",
    "* Sparse solution (solution of a problem where most of the elements are zero)\n",
    "* Reason: Samples only appear as pairwise inner products.\n",
    "* Sparsity desired property because it often leads to simpler, more interpretable models.\n",
    "* QPP - Quadratic programming problem\n",
    "\n",
    "Dual from of the decision function:\n",
    "\n",
    "$$\n",
    "f_{\\beta}(x)= \\sum_{x_i\\in SV} \\beta_i y_i k(x_i, x)\n",
    "$$\n",
    "\n",
    "(SV = Support Vectors)\n",
    "\n",
    "* Only the support vectors (points with non-zero beta_i) contribute to the decision function.\n",
    "* Decision function is weighted sum over the support vectors.\n",
    "* Decides the class based on the sign of this sum.\n",
    "\n",
    "\n",
    "**Hint:**\n",
    "\n",
    "This code is a implementation of kernelized empirical risk minimization that aligns with the SVM concepts but uses gradient descent instead of directly solving the dual problem via quadratic programming."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
