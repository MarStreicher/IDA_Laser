{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear classifier\n",
    "\n",
    "Classifier: SVM\n",
    "\n",
    "$$\n",
    "\\argmin_{\\theta} \\sum_{i=1}^n l(max(0,1-y_if_{\\theta}(x_i)),y_i) + \\lambda \\Omega_2(\\theta)\n",
    "$$\n",
    "\n",
    "- Hinge loss \n",
    "- L2 Regularizer (Squared Euclidean Norm)\n",
    "- Numeric solution using Stochastic Gradient Descent Method (SGD)\n",
    "- ERM is about the *objective* (minimizing the empirical risk), while gradient descent is about the *method* (an algorithm to find the parameters that minimize the empirical risk)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Calculate stopping criterion\n",
    "\n",
    "$$\n",
    "\\parallel \\Theta^t - \\Theta^{t+1} \\parallel > \\epsilon\n",
    "$$ \n",
    "\n",
    "L2 Norm is used, hence:\n",
    "\n",
    "$$\n",
    "\\|\\Theta^t - \\Theta^{t+1}\\|_2 = \\sqrt{(\\Theta^t_1 - \\Theta^{t+1}_1)^2 + (\\Theta^t_2 - \\Theta^{t+1}_2)^2 + \\cdots + (\\Theta^t_n - \\Theta^{t+1}_n)^2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Claculate gradient for Hinge loss and L2 Regularizer\n",
    "\n",
    "Hinge loss with linear model:\n",
    "\n",
    "$$\n",
    "max(0, 1-y_i(x_i*\\theta_i))\n",
    "$$\n",
    "\n",
    "Gradient for a single instance:\n",
    "\n",
    "$$\n",
    "\\nabla_{\\theta} L(\\theta) = \\frac{\\partial l(max(0, 1-y_i(x_i*\\theta)))}{\\partial \\theta} + \\frac{\\lambda}{n}\\frac{\\partial \\Omega_2(\\theta)}{\\partial \\theta}\n",
    "$$\n",
    "\n",
    "Partial derivation of the Hinge loss:\n",
    "\n",
    "$$\n",
    "\\nabla_{\\theta} L(\\theta) = \n",
    "\\begin{cases}\n",
    "0, & \\text{if } 1 - y_i (\\mathbf{x}_i \\cdot \\theta) \\leq 0 \\\\\n",
    "-y_i \\mathbf{x}_i, & \\text{if } 1 - y_i (\\mathbf{x}_i \\cdot \\theta) > 0 \n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "\n",
    "Partial derivation of the L2 Regularizer:\n",
    "$$\n",
    "\\nabla_{\\theta} \\Omega_2(\\theta) = \\theta\n",
    "$$\n",
    "\n",
    "Hence, combination:\n",
    "$$\n",
    "\\nabla_{\\theta} L(\\theta) = \n",
    "\\begin{cases}\n",
    "\\frac{\\lambda}{n}\\theta, & \\text{if } 1 - y_i (\\mathbf{x}_i \\cdot \\theta) \\leq 0 \\\\\n",
    "-y_i \\mathbf{x}_i + \\frac{\\lambda}{n}\\theta, & \\text{if } 1 - y_i (\\mathbf{x}_i \\cdot \\theta) > 0 \n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Calculate step size alpha\n",
    "\n",
    "$$\n",
    "\\alpha(t) = \\frac{\\alpha_0}{1 + \\text{decay\\_rate} \\cdot t}\n",
    "$$\n",
    "\n",
    "## Learn the classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "import torch\n",
    "import pickle\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from linear_classifier_helper import LinearClassifierHelper\n",
    "from dataset import BaseDataset, FeatureEngineeredDataset\n",
    "\n",
    "file_path = \"../../data/laser.mat\"\n",
    "mat_dict = loadmat(file_path)\n",
    "\n",
    "dataset = FeatureEngineeredDataset(mat_dict, \"X\", \"Y\", \"r2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# param_grid = {\n",
    "#     'lambda_value': [0.001, 0.01, 0.1, 1.0, 10.0],\n",
    "#     'alpha_0': [0.001, 0.01, 0.1],\n",
    "#     'decay_rate': [0.001, 0.01, 0.1],\n",
    "#     'epsilon': [1e-4, 1e-5, 1e-6]\n",
    "# }\n",
    "\n",
    "# def train_model_with_params(params):\n",
    "#     return LinearClassifierHelper.reg_erm_stoch(dataset.inputs, dataset.labels, \n",
    "#                                                 lambda_value=params['lambda_value'], \n",
    "#                                                 epsilon=params['epsilon'], \n",
    "#                                                 alpha_0=params['alpha_0'], \n",
    "#                                                 decay_rate=params['decay_rate'])\n",
    "\n",
    "# best_params = None\n",
    "# best_score = float('-inf')\n",
    "\n",
    "# for params in ParameterGrid(param_grid):\n",
    "#     score = train_model_with_params(params)\n",
    "#     if score > best_score:\n",
    "#         best_score = score\n",
    "#         best_params = params\n",
    "\n",
    "# print(\"Best parameters found:\", best_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.38743536]\n"
     ]
    }
   ],
   "source": [
    "optimal_theta = LinearClassifierHelper.reg_erm_stoch(dataset.inputs, dataset.labels, lambda_value = 0.001, epsilon = 1e-05, alpha_0 = 0.1, decay_rate = 0.001)\n",
    "print(optimal_theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n"
     ]
    }
   ],
   "source": [
    "raw_predictions = LinearClassifierHelper.predict(dataset.inputs, optimal_theta)\n",
    "\n",
    "accuracy = LinearClassifierHelper.evaluate(raw_predictions, dataset.labels)\n",
    "print(accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envPython3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
